% !TEX root = growth.tex
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Derivation and estimation of a fluorescence calibration factor}
Griffin Chure - \today


\subsection*{Derivation}

\hspace{10mm} Imagine that we have a cell that has a fixed number of fluorescent proteins.
As this cell divides, these proteins will be Binomially partitioned into the
two daughter cells with some probability $p$ dictating the proteins' likelihood
to end up in one cell over the other. By examining the difference in
fluorescence between the two daughters, we can determine just how bright a
single molecule is on average.

We begin by positing that the fluorescence is conserved from mother to
daughters, where we assume that production and degradation rates of the
fluorophore are negligible. Mathematically, we can state this as
\begin{equation}
I_\text{tot} = I_1 + I_2
\label{eq:fluo_cons}
\end{equation}
where $I_\text{tot}$ is the total fluorescence of the mother cell and $I_1$ and
$I_2$ are the intensities of the two daughter cells. If we assume that the
measured fluorescence arises only from the fluorophores, we say that the
total intensity of the mother cell should be proportional to the number of
fluorescent proteins $N_\text{tot}$,
\begin{equation}
I_\text{tot} = \alpha N_\text{tot}.
\label{eq:antot}
\end{equation}
This can be similarly stated for each individual daughter cell,
\begin{equation}
I_1 = \alpha N_1\,\, ; \,\, I_2 = \alpha N_2
\end{equation}
which follows the assumption that the protein copy number is conserved as well,
\begin{equation}
N_\text{tot} = N_1 + N_2.
\end{equation}
When a cell divides, the probability distribution $g(n)$  of finding $n$ proteins
in a daughter cells is Binomial with a partitioning probability $p$,
\begin{equation}
g(n \, \vert \, N_\text{tot}) = {N_\text{tot}! \over n! (N_\text{tot} - n)!} p^{n}(1 - p)^{N_\text{tot} - n}.
\label{eq:binomial}
\end{equation}
To examine how the proteins were partitioned amongst the daughter cells, we can
look at how different the intensities are between them and relate it to the
total intensity of the mother as
\begin{equation}
\langle(I_1 - I_2)^2\rangle = \langle (2I_1 - I_\text{tot})^2 \rangle
\end{equation}
which we can translate to protein copy numbers as
\begin{equation}
\langle(I_1 - I_2)^2\rangle = \langle\left(2\alpha N_1 - \alpha N_\text{tot}\right)^2\rangle.
\end{equation}
As we are discussing averages in this context, we can examine this more generally as
\begin{equation}
\langle(I_1 - I_2)^2 \rangle = (2\alpha\langle n \rangle - \alphaN_\text{tot})^2.
\end{equation}
Expanding this yields
\begin{equation}
  \langle (I_1 - I_2)^2 \rangle = 4\alpha^2\langle n^2 \rangle - 4\alpha\langle n \rangle N_\text{tot} + \alpha^2 N_\text{tot}^2.
\end{equation}

To move forward, we must know the two moments of the Binomial distribution, $\langle n \rangle$ and $\langle n^2 \rangle$. While we likely have these committed to memory by now, it's always
a useful exercise to be explicit and will derive them below. The mean number of proteins partitioned into one daughter $\langle n \rangle$ can be computed as
\begin{equation}
\langle n \rangle = \sum\limits_{n = 0}^{N_\text{tot}}n{{N_\text{tot}}\choose{n}}p^{n}(1-p)^{N_\text{tot} - n}.
\label{eq:avg_n}
\end{equation}
We can examine the product of $n$ and the Binomial coefficient and see that it can be simplified as
\begin{equation}
n {N_\text{tot}! \over n!(N_\text{tot} - n)!} = {N_\text{tot}! \over (n-1)!(N_\text{tot} - n)!} = {N_\text{tot}(N_\text{tot} - 1)! \over (n - 1)!(N_\text{tot} - n)!}.
\label{eq:expanded_binom}
\end{equation}
Plugging this back into Eq. \ref{eq:avg_n}, we have
\begin{equation}
\langle n \rangle = N_\text{tot} p \sum\limits_{n= 1}^{N_\text{tot}}{{N_\text{tot} - 1}\choose{n - 1}}p^{n - 1}(1 - p)^{N_\text{tot} - n}.
\end{equation}
By defining $\nu = N_\text{tot} - 1$ and $k = n - 1$, we yield.
\begin{equation}
\langle n \rangle = N_\text{tot} p \overbrace{\sum\limits_{k= 0}^{\nu}{{\nu}\choose{k}}p^k(1 - p)^{\nu - k}}^\text{1} = N_\text{tot} p
\label{eq:reparam_binom}
\end{equation}
which is what we would na√Øvely expect.


We will also need to know the sTo compute the second moment, $\langle n^2 \rangle$, it's easier to compute
the variance
realizing
\begin{equation}
\langle n^2 \rangle = N_\text{tot}p\sum\limits_{n=1}^{N_\text{tot}}n{{N_\text{tot} - 1}\choose{n -1}}p^{n-1}(1 - p)^{N_\text{tot} - n} = N_\text{tot}p\sum\limits_{k=0}^\nu(k + 1){{\nu}\choose{k}}p^k(1-p)^{\nu - k}  \tag{14},
\label{eq:avg_nsq}
\end{equation}
where we pull the same trick of reparameterizing \ref{eq:reparam_binom} for $\nu = N_\text{tot} - 1$ and $k = n - 1$. Simplifying \ref{eq:avg_nsq} generates
\begin{equation}
\langle n^2 \rangle = (N_\text{tot}p)^2 + N_\text{tot}p(1 - p) \tag{15}
\end{equation}
allowing us to express the variance $\sigma^2$ as
\begin{equation}
\sigma^2 = \langle n^2 \rangle - \langle n \rangle^2 = N_\text{tot}p(1 - p).
\end{equation}
We can make our calculation a bit less verbose by assuming that partitioning
of the proteins is always fair such that $p = 0.5$. This means our mean and
variance can be more simply written as
\begin{equation}
\langle n \rangle = {N_\text{tot} \over 2}\, ; \, \langle n^2 \rangle= {N_\text{tot}+ N_\text{tot}^2 \over 4}.
\label{eq:mean_var}
\end{equation}

We can now return to Eq. \ref{eq:avg_n} and include the mean copy number,
\begin{align}
\langle (I_1 - I_2)^2 \rangle &= 4\alpha^2(\langle n^2 \rangle - \langle n \rangle N_\text{tot}) + (\alpha N_\text{tot})^2\\
& = 4\alpha^2\left({N_\text{tot} + N_\text{tot}^2 \over 4} - {2N_\text{tot}^2 \over 4}\right) + \alpha^2N_\text{tot}^2.
\end{align}
Some simplification brings us to our result,
\begin{equation}
\langle (I_1 - I_2)^2 \rangle = \alpha^2 N_\text{tot} = \alpha I_\text{tot}.
\label{eq:golden_rule}
\end{equation}
This result tells us that the squared difference in the intensity between any
two daughter cells should be linearly related to the intensity of the mother
cell with a slope of the calibration factor $\alpha$, as is confirmed through
simulation in Fig. \ref{fig:dilution_sim}.


\begin{figure}
  \centering{
  \includegraphics{../figs/simulated_dilution_simple.pdf}
  \label{fig:dilution_sim}
  \caption{\small\textbf{Simulated protein partitioning with no measurement noise.}
  Simulation performed with mother protein copy numbers evenly spaced from
  10 to 1000 in intervals of 5. Each partitioning event was simulated 100 times
  and averages were computed. Intensities were calculated by multiplying the
  number of partitioned proteins by a set calibration factor $\alpha = 150$ a.u.
  per molecule. Black points correspond to individual simulations, red points
  are the averages over all simulations, and the blue line is the prediction
  given in Eq. \ref{eq:golden_rule}.}
  }
\end{figure}


\subsection*{Bayesian parameter estimation of $\alpha$}
Given a data setwhere we have all of the fluorescence information across lineages,
we must estimate the best-fit value of $\alpha$. This has typically been performed
by combining a given number of division events into bins, computing the relevant
averages, and then performing simple linear regression to find the best fit estimate
for the calibration factor. While this is a valid approach, a major disadvantage
is the arbitrary decision of bin width. To ensure you are estimating the ``right"
value of $\alpha$, you must perform the regression of a wide range of bin widths
and look for convergence.

Rather than pooling data together, here we will take a Bayesian approach where
each partitioning event is considered individually.

We can begin by writing Bayes' theorem,
\begin{equation}
g(\alpha\, \vert \, I_1, I_2) \propto f(I_1, I_2 \,\vert\, \alpha) g(\alpha),
\label{eq:bayes_thm}
\end{equation}

where $I_1$ and $I_2$ are the intensities of two daughter cells arising from
a single division event. To avoid overloading of the operator $P$, I have denoted
probability densities describing measured data with $f$ and those describing
parameter values with $g$.

As we know that $I_2$ is related to $I_1$ from Eq. \ref{eq:fluo_cons}, we can
simplify the likelihood in Eq. \ref{eq:bayes_thm} as
\begin{equation}
g(\alpha \, \vert \, I_1, I_2) \propto f(I_1\, \vert\, I_2, \alpha) f(I_2 \, \vert\, \alpha) g(\alpha).
\end{equation}

We can treat the priors $g(I_2 \,\vert\, \alpha)$ and $g(\alpha)$ as improper
uniform (as they could be anything), our posterior becomes proportional to our
likelihood. We now need to develop a statistical model that describes the
intensity of one daughter cell given the other intensity and a calibration
factor. While this is not necessarily obvious, we know the statistical model
for the partitioning of the proteins should be binomial. Through change of
variables, we can rewrite our likelihood in terms of protein copy number as
\begin{equation}
f(I_1\,\vert\, I_2, \alpha) = f(N_1\,\vert\, \alpha) \left\vert {dN_1 \over dI_1} \right\vert.
\label{eq:cov}
\end{equation}

Given Eq. \ref{eq:antot}, the derivative in Eq. \ref{eq:cov} can be calculated as
\begin{equation}
{dN_1 \over dI_1} = {d \over dI_1} {I_1 \over \alpha} = {1 \over \alpha}.
\end{equation}

Our likelihood can now be written as
\begin{equation}
g(\alpha\,\vert\, I_1, I_2) = {1 \over \alpha} f(N_1\,\vert\, I_2, \alpha).
\end{equation}

As $N_1$ is Binomially distributed from a pool $N_\text{tot}$,  the likelihood becomes
\begin{equation}
f(N_1 \,\vert\, I_2, \alpha) = {1\over \alpha}{N_\text{tot}! \over N_1! N_2!}p^{N_1}(1 - p)^{N_2}
\label{eq:bayes_binom}
\end{equation}

If we take the deterministic assumption of our model that
\begin{equation}
N_1 = {I_1 \over \alpha},
\end{equation}

and taking partitioning to be fair ($p = 0.5$), the posterior becomes
\begin{equation}
g(\alpha\,\vert\,I_1, I_2) = {1 \over \alpha}{{I_1 + I_2 \over \alpha}! \over {I_1 \over \alpha}! {I_2 \over \alpha}!}2^{-{I_1 + I_2 \over \alpha}}.
\label{eq:improper_posterior}
\end{equation}

The factorials in Eq. \ref{eq:improper_posterior} are improper intensity is not
a discrete quantity and therefore has an undefined factorial. However, these
factorials can be approximated through gamma functions (where continuous variables
are calculable) as
\begin{equation}
n! \approx n\Gamma(n) = \Gamma(n + 1).
\label{eq:gamma_approx}
\end{equation}

Applying Eq. \ref{eq:gamma_approx} to Eq. \ref{eq:improper_posterior} our
complete posterior for a single division event is given by
\begin{equation}
  g(\alpha\,\vert\, I_1, I_2)= {1 \over \alpha}{\Gamma({I_1 + I_2 \over \alpha} + 1) \over \Gamma({I_1 \over \alpha} +1 )\Gamma({I_2 \over \alpha} + 1)}2^{-{I_1 + I_2 \over \alpha}}.
\end{equation}

For full set of $k$ divisions,
\begin{equation}
  g(\alpha\, \vert\, [I_1, I_2]) ={1 \over \alpha^k}\prod\limits_i^k {\Gamma({I_{1,i} + I_{2,i} \over \alpha} + 1) \over \Gamma({I_{1,i} \over \alpha} + 1) \Gamma({I_{2,i} \over \alpha} + 1)}2^{-{I_{1,i} + I_{2,i} \over \alpha }}.
\label{eq:posterior}
\end{equation}

Eq. \ref{eq:posterior} can be solved for a given data set through optimization
quite easily. In this work, we will sample directly out of this distribution
using Markov Chain Monte Carlo (MCMC). This decision was made to obtain a more
clear error estimate for $\alpha$. If being solved by minimization, we obtain
a statistical error estimate by approximating the posterior distribution as a
Gaussian distribution and computing the Hessian matrix. Through MCMC, we can
simply compute the credible region, which is the narrowest window which contains
95\% of the probability. An example posterior distribution for $\alpha$ using
the data from Fig \ref{fig:dilution_sim} can be seen in Fig. \ref{fig:param_estimation}.

\begin{figure}[h!]
  \centering{
  \includegraphics{../figs/alpha_simple_mcmc.pdf}
  \label{fig:param_estimation}
  \caption{\small\textbf{Posterior probability distribution for $\alpha$.} (A) The improperly
  normalized posterior probability distribution $g(\alpha\, \vert\, [I_1, I_2])$
  from Eq. \ref{eq:posterior} for data shown in Fig. \ref{fig:dilution_sim}.
  The mode and bounds of the 95$^\text{th}$ percentile of the credible region
  is shown by a red circle and line respectively. (B) The steps of
  the MCMC walker. MCMC was performed using NUTS sampler with
  10000 tuning steps and 10000 draws. The best-fit parameter value for $\alpha$
  is $153^{+3}_{-3}$ a.u. per molecule.
  }
  }
\end{figure}

\subsection*{Inclusion of noise}
Up to this point, we have considered a perfect experimental system in which
there is zero noise in the measurement of the cellular intensities. While there
are other assumptions such as neglecting of degradation, inactive or misfolded
fluorophores, and leaky production, the exclusion of noise in the measurement
is likely the largest contributor to the inaccuracy of the model. In this section,
we will attempt to describe several different models of noise in the intensity
measurement as well as investigate their effects on the resulting estimate of
$\alpha$.

\subsubsection*{Model I: Measurement error}

Since we don't live in a perfect world, our instrumentation can't make perfect
measurements. While modern camera technology has greatly improved our ability
to measure weak signal, we always have to battle the stochastic nature of photon
counting and varying autofluorescence.

We can include this source of noise into our model by modifying Eq. \ref{eq:antot}
as
\begin{equation}
  I_\text{tot} = \alpha N_\text{tot} + \epsilon,
  \label{eq:model1}
\end{equation}

where $\epsilon$ can be described by a normal distribution with zero mean and
a variance $\sigma^2$.

Of particular interest is the error that results from using the posterior show in Eq. \ref{eq:posterior}
to estimate the value of $\alpha$ given the stipulation in Eq. \ref{eq:model1}. {\textcolor{red}{I
want to derive the analytical solution to this but don't have time right now.}} This calculation
is shown in Fig. \ref{fig:model_1_err} in which the experiment was simulated over a range of values
of $\sigma$. The approximate error of three illumination methods are also shown in Fig. \ref{fig:model_1_err}(B). Large differences in the estimated value of $\alpha$ compared to the
true value only appear when the error reaches around 1\%.

\begin{figure}[h]
  \centering{
  \includegraphics{../figs/error_est_model1.pdf}
  \label{fig:model_1_err}
  \caption{\textbf{Numerical calculations of error resulting from neglecting
  measurement error.} An experiment was simulated over a range of 10 to 1000
  proteins at separated by intervals of 5. Each simulation was performed 100
  times for statistical convergence. The parameter estimation was performed via
  optimization of Eq. \ref{eq:posterior} rather than MCMC for computational
  ease. (A) The estimated value of $\alpha$ as a function of the percent error.
  The blue line represents the ``true" seeded value of $\alpha$. (B)
  Overestimate factor of the estimated $\alpha$ value relative to the true
  value. Red, gray, and blue lines represent the approximate errors of a
  typical mercury lamp, white light LED, and a laser, respectively.} }
\end{figure}

As was performed in the fist section, we can write down a scheme for estimating
the parameters. Our posterior distribution is now very different and requires
complete revision. We now have to estimate both $\alpha$ and $\sigma$. Using
general terms, we can write Bayes' theorem,
\begin{equation}
  g(\alpha, \sigma\, \vert\, I_1, I_2) \propto f(I_1, I_2\,\vert\, \alpha, \sigma)g(\alpha)g(\sigma).
  \label{model1_bayes}
\end{equation}

Here, we have assumed that $\alpha$ and $\sigma$ are independent parameters
allowing us to separate their prior distributions by the product rule. To start
off easy, we can take the prior on $\alpha$ to be anything, so long as it is positive.
We can therefore assign an improper uniform prior and drop it from our final
posterior. For $\sigma$, we can use a Jeffrey's prior (as is typically used
for scale parameters),
\begin{equation}
  g(\sigma) = {1 \over \sigma}.
\end{equation}

Our likelihood, $f(I_1, I_2\, \vert\, \alpha, \sigma)$ is slightly more complicated.
We cannot take the same deterministic approach as we did in Eq. \ref{eq:cov}.
We use our assumption that the noise is normally distributed, allowing us
to take a Gaussian distribution as our likelihood,
\begin{equation}
  f(I_1\,\vert \, \alpha, \sigma, N_1)= {1 \over \sqrt{2\pi\sigma^2}}\exp\left[-{(I_1 - \alpha N_1)^2 \over 2\sigma^2}\right].
  \label{eq:model1_like_i1}
\end{equation}

Similarly, the likelihood for $I_2$ can be written in terms of $N_1$,
\begin{equation}
  f(I_2\,\vert\, \alpha, \sigma, N_1) = {1 \over \sqrt{2\pi\sigma^2}}\exp\left[-{(I_2 - \alpha (N_\text{tot} - N_1))^2 \over 2\sigma^2}\right].
\end{equation}
We can then write our complete posterior as
\begin{equation}
  g(\alpha, \sigma\, [N_1, N_\text{tot}]\, \vert\, [I_1, I_2]) = {1\over
  (2\pi)^{k/2}\sigma^{k+1}}
  \prod\limits_i^k\exp\left[-{(I_{1,i} - \alpha
  N_{1,i})^2 - (I_{2,i} - \alpha(N_{\text{tot},i} - N_{1,i}))^2 \over
  2\sigma^2}\right].
  \label{eq:model1_post}
\end{equation}

This posterior cannot be solved analytically, but can be sampled using MCMC. However, this becomes
very computationally intensive very quickly. With $k$ division events, we have $2k + 2$ parameters
that need to be sampled.


\subsubsection*{Model II: Spatial variation}
We often assume that the intensity of the excitation illumination is uniform across our field of
view, even if we've corrected for the Gaussian profile of incident light. However, coherence in
the excitation beam can produce fringes on the scale of the wavelength, which quite unfortunately
happens to be on the length scale of a single cell. This means that any cells resting on one of
these fringes will have different values of $\alpha$ for different regions of the cell. We can
examine the extreme case in which every cell rests on a fringe. We can incorporate this source
of noise into our model as

\begin{equation}
  I_\text{tot} = \sum\limits_i^{N_\text{tot}} \alpha_i,
\end{equation}

where $\alpha_i$ is normally distributed with a mean of $\alpha$ and a variance $\sigma^2$.

We again would like to investigate the effect that neglecting this variation has on our ultimate
estimation of $\alpha$.
\textcolor{red}{Again, I want to solve this analytically.} A numerical calculation of this effect
is shown in Fig. \ref{fig:model2_err}. Even large variations in this intensity only marginally
impacts the estimation of $\alpha$ up to a few percent variation. \textcolor{red}{I need to work
on this more -- I think this is wrong. This would mean a cell that has more protein has more
fringes. I need to come up with some way to break it up in chunks.}

\begin{figure}
  \centering{
  \includegraphics{../figs/err_est_model2.pdf}
  \label{fig:model2_err}
  \caption{\textbf{Numerical calculation of error resulting from neglecting spatial variance.} (A)
  Direct estimate of $\alpha$ over a range of errors. (B) The overestimation factor from neglecting
  the spatial variance.
  }
  }
\end{figure}


\subsubsection*{Model III: Intensity variation}
While the illumination from a flashlight or a laser pointer may appear to be a fixed value, there
are often high-frequency variations in the irradiance, often referred to as the intensity.

We can imagine that in addition to being perfectly folded and indestructible, we can imagine that
they also have 100\% quantum efficiency and their excitation cannot saturate. The intensity of the
emitted light is linear proportional to the intensity of the excitation beam $\Theta$ and the
exposure time $\tau$,
\begin{equation}
  I \propto \Theta \tau.
  \label{eq:intensity_time}
\end{equation}

A result of this assumption is rather intuitive -- a decrease in intensity results in a lower signal,
and therefore a lower value of $\alpha$. Conversely, a gain in intensity increases the calibration
factor. As this is a linear relationship, the intensity becomes a multiplicative factor of $\alpha$.
Using Eq. \ref{eq:intensity_time}, we can model this effect as

\begin{equation}
  I_\text{tot} = \bar{\Theta}\alpha N_\text{tot},
\end{equation}

where \bar{\Theta} is a normalized intensity distribution which can be assumed to be Gaussian with
unity mean and a variance $\sigma^2$. As this is multiplicative, one would expect that smaller
levels of variation should result in overestimation of $\alpha$ than in the previous two models.
\textcolor{red}{Analytical solution here}. Fig. \ref{fig:err_est_model3} shows a numerical calculation
of this error over a range of intensity variations. Panels (A) and (B) show the full scale of the
effect with very large overestimates of the calibration factor appear when the percent variation is
above 1\%.

\begin{figure}
  \centering{
  \includegraphics{../figs/err_est_model3.pdf}
  \label{fig:err_est_model3}
  \caption{
  \textbf{Numerical calculation of error resulting from neglecting temporal variation.} The simulation
  was performed as described previously. (A) The estimated value of $\alpha$ as a function of intensity
  variation. (B) The same data from (A) with a smaller scaling on the $y$ axis. (C) The relative
  overestimate of $\alpha$ with compared to the true value. (D) Same data as (C) with different
  scaling on the $y$ axis.
  }
  }
\end{figure}

\subsubsection*{Model IV: Combined error}
The worst of both worlds -- coming soon to a \TeX document near you. I think this will be the
closest to the actual experimental situation.
